{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing the easy way\n",
    "## or\n",
    "## How to boost performance by at least 300% in 5 minutes\n",
    "\n",
    "In this tutorial we are looking at an easy way to implement CPU-based parallelization for 'embarassingly parallel' problems (see the [wikipedia article](https://en.wikipedia.org/wiki/Embarrassingly_parallel) for details). In short 'embarassingly parallel' means that the individual processes run independent of each other and there is no need for communication.\n",
    "\n",
    "As an example, say you are running three different IAMs for three different scenarios each. None of these calculations influence each other in any way. In principle they can be run in arbitrary order. \n",
    "\n",
    "Generally speaking, the chace is high that you are looking at an emabarassingly parallel task whenever you have something like the following pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 12.01 seconds\n",
      "[\"model = 'MESSAGE', scenario = 'Current Policies'\", \"model = 'MESSAGE', scenario = '1.5C target'\", \"model = 'GCAM', scenario = 'Current Policies'\", \"model = 'GCAM', scenario = '1.5C target'\", \"model = 'REMIND', scenario = 'Current Policies'\", \"model = 'REMIND', scenario = '1.5C target'\"]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def some_computation(model, scenario):\n",
    "    time.sleep(2) # simulate some expensive calculation for the model, scenario combination\n",
    "    # return what we have calculated so that we \n",
    "    return f'{model = }, {scenario = }'\n",
    "\n",
    "models = [\"MESSAGE\", \"GCAM\", \"REMIND\"]\n",
    "scenarios = [\"Current Policies\", \"1.5C target\"]\n",
    "\n",
    "res = []\n",
    "start_time = time.time()\n",
    "for m in models:\n",
    "    for s in scenarios:\n",
    "        res.append(some_computation(m, s))\n",
    "            \n",
    "print(f'That took {time.time()-start_time:.2f} seconds')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Notes:\n",
    "\n",
    "* The function `some_computation` can of course have an arbitrary amount of arguments. \n",
    "* Feeding `m` and `s` into the function using a double loop is of course not the most efficient way (using something like [`itertools.product`](https://docs.python.org/3/library/itertools.html#itertools.product) from the python standard library is much better). However, it should illustrate the point that the individual iterations have no dependency.\n",
    "\n",
    "## Using [`joblib.Parallel`](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html) to speed up computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 2.01 seconds\n",
      "[\"model = 'MESSAGE', scenario = 'Current Policies'\", \"model = 'MESSAGE', scenario = '1.5C target'\", \"model = 'GCAM', scenario = 'Current Policies'\", \"model = 'GCAM', scenario = '1.5C target'\", \"model = 'REMIND', scenario = 'Current Policies'\", \"model = 'REMIND', scenario = '1.5C target'\"]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "start_time = time.time()\n",
    "res = Parallel(n_jobs=-1)(delayed(some_computation)(m, s) for m in models for s in scenarios)\n",
    "# alternatively if we want to combine every model with every scenario we \n",
    "# can also use itertools.product. In this case, however, we need to unpack\n",
    "# the tuple that itertools.product gets us\n",
    "# res = Parallel(n_jobs=-1)(delayed(some_computation)(*arg_tuple) for arg_tuple in itertools.product(models,scenarios))\n",
    "print(f'That took {time.time()-start_time:.2f} seconds')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at the parallel implementation\n",
    "\n",
    "There are three main componentens to using `joblib` to parallelize computation:\n",
    "\n",
    "1. **Using the `Parallel` object**: `Parallel` can take a number of parameters, the most important being `n_jobs`. This specifies the number of parallel processes to run. If `n_jobs=-1` all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used (from the [joblib documentation](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html)). In the documentation information about the additional input parameters and more examples can be found.\n",
    "2. **Wrapping the function in the `delayed` decorator**: This step involves nothing more than, in our case, just calling `delayed(some_computation)`.\n",
    "3. **Feeding in the input values**: Finally we need to 'feed' the data for each iteration into the function. In the previous example we did this using a list comprehension. `(m, s) for m in models for s in scenarios`. In this example and in the example using `itertools.product` we fed the data into the function as positional arguments. Sometimes it can be convenient to add them as keyword arguments. This can be the case if we have a function that has some default arguments which we do not want to change when running the function. For example if we have a function `def f(a,b,c=3,d=5)` and we want to use the default `c=3` but change the input d. We need to call the function with keyword arguments like this `f(a=1, b=3, d= 17)`. This can also be achieved using `joblib.Parallel` as the following exmple illustrates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_kwargs = ({'model':m, 'scenario':s} for m in models for s in scenarios)\n",
    "res = Parallel(n_jobs=-1)(delayed(some_computation)(**kwargs) for kwargs in input_dict)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return Values\n",
    "\n",
    "As seen in the examples, the results of the parallel executions are simply returned as a list of all return values. If the parallel function we are running returns for example a pandas DataFrame it can be useful to use the following pattern:\n",
    "```python\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
