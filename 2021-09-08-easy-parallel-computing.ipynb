{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing the easy way\n",
    "## or\n",
    "## How to boost performance by 300% in 5 minutes (\\*)\n",
    "\n",
    "(\\*) depends on how many cores you have and the first implementation is probably going to take longer than 5 minute, but I wanted a catchy title.\n",
    "\n",
    "---\n",
    "In this tutorial we are looking at an easy way to implement CPU-based parallelization for 'embarrassingly parallel' problems (see the [wikipedia article](https://en.wikipedia.org/wiki/Embarrassingly_parallel) for details). In short 'embarrassingly parallel' means that the individual processes run independent of each other and there is no need for communication.\n",
    "\n",
    "As an example, say you are running three different IAMs for three different scenarios each. None of these calculations influence each other in any way. In principle they can be run in arbitrary order. \n",
    "\n",
    "Generally speaking, the chance is high that you are looking at an embarrassingly parallel task whenever you have something like the following pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 6.03 seconds\n",
      "[\"model = 'MESSAGE', scenario = 'Current Policies'\", \"model = 'MESSAGE', scenario = '1.5C target'\", \"model = 'GCAM', scenario = 'Current Policies'\", \"model = 'GCAM', scenario = '1.5C target'\", \"model = 'REMIND', scenario = 'Current Policies'\", \"model = 'REMIND', scenario = '1.5C target'\"]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def some_computation(model, scenario):\n",
    "    # simulate some expensive calculation for the model, scenario combination\n",
    "    time.sleep(1)\n",
    "    # return what we have calculated so that we \n",
    "    return f'{model = }, {scenario = }'\n",
    "\n",
    "models = [\"MESSAGE\", \"GCAM\", \"REMIND\"]\n",
    "scenarios = [\"Current policies\", \"1.5C target\"]\n",
    "\n",
    "res = []\n",
    "start_time = time.time()\n",
    "for m in models:\n",
    "    for s in scenarios:\n",
    "        res.append(some_computation(m, s))\n",
    "            \n",
    "print(f'That took {time.time()-start_time:.2f} seconds')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Notes:\n",
    "\n",
    "* The function `some_computation` can have an arbitrary amount of arguments. \n",
    "* Feeding `m` and `s` into the function using a double loop is not the most efficient way. Using e.g. [`itertools.product`](https://docs.python.org/3/library/itertools.html#itertools.product) from the python standard library is much better. However, it should illustrate the point that the individual iterations have no dependency.\n",
    "\n",
    "## Using [`joblib.Parallel`](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html) to speed up computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 2.52 seconds\n",
      "[\"model = 'MESSAGE', scenario = 'Current Policies'\", \"model = 'MESSAGE', scenario = '1.5C target'\", \"model = 'GCAM', scenario = 'Current Policies'\", \"model = 'GCAM', scenario = '1.5C target'\", \"model = 'REMIND', scenario = 'Current Policies'\", \"model = 'REMIND', scenario = '1.5C target'\"]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "start_time = time.time()\n",
    "res = Parallel(n_jobs=-1)(delayed(some_computation)(m, s) for m in models for s in scenarios)\n",
    "# using itertools.product we need to unpack the output tuple.\n",
    "# res = Parallel(n_jobs=-1)(delayed(some_computation)(*arg_tuple) for arg_tuple in itertools.product(models,scenarios))\n",
    "print(f'That took {time.time()-start_time:.2f} seconds')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at the parallel implementation\n",
    "\n",
    "There are three main componentens to using `joblib` to parallelize computation:\n",
    "\n",
    "1. **Using the `Parallel` object**: `Parallel` can take a number of parameters, the most important being `n_jobs`. This specifies the number of parallel processes to run. If `n_jobs=-1` all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used (from the [joblib documentation](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html)). In the documentation information about the additional input parameters and more examples can be found.\n",
    "2. **Wrapping the function in the `delayed` decorator**: This step involves nothing more than, in our case, just calling `delayed(some_computation)`.\n",
    "3. **Feeding in the input values**: Finally we need to 'feed' the data for each iteration into the function. In the previous example we did this using a list comprehension. `(m, s) for m in models for s in scenarios`. In this example and in the example using `itertools.product` we fed the data into the function as positional arguments. Sometimes it can be convenient to add them as keyword arguments. This can be the case if we have a function that has some default arguments which we do not want to change when running the function. For example if we have a function `def f(a,b,c=3,d=5)` and we want to use the default `c=3` but change the input d. We need to call the function with keyword arguments like this `f(a=1, b=3, d= 17)`. This can also be achieved using `joblib.Parallel` as the following exmple illustrates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"model = 'MESSAGE', scenario = 'Current Policies'\", \"model = 'MESSAGE', scenario = '1.5C target'\", \"model = 'GCAM', scenario = 'Current Policies'\", \"model = 'GCAM', scenario = '1.5C target'\", \"model = 'REMIND', scenario = 'Current Policies'\", \"model = 'REMIND', scenario = '1.5C target'\"]\n"
     ]
    }
   ],
   "source": [
    "input_kwargs = ({'model':m, 'scenario':s} for m in models for s in scenarios)\n",
    "res = Parallel(n_jobs=-1)(delayed(some_computation)(**kwargs) for kwargs in input_kwargs)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return values\n",
    "\n",
    "As seen in the examples, the results of the parallel executions are simply returned as a list of all return values. If the parallel function we are running returns for example a pandas DataFrame it can be useful to use the following pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.949969</td>\n",
       "      <td>0.539793</td>\n",
       "      <td>0.192273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.947121</td>\n",
       "      <td>0.418268</td>\n",
       "      <td>0.338098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.726021</td>\n",
       "      <td>0.846280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c\n",
       "1  0.949969  0.539793  0.192273\n",
       "2  0.947121  0.418268  0.338098\n",
       "3  0.146354  0.726021  0.846280"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "i=0\n",
    "\n",
    "def return_pandas_df():\n",
    "    # the global variable i acts as a counter so that the \n",
    "    # index of the dataframe corresponds to how many times\n",
    "    # the function was called in total\n",
    "    global i \n",
    "    i+= 1\n",
    "    return pd.DataFrame(np.random.rand(1,3), columns=('a', 'b', 'c'), index=[i])\n",
    "\n",
    "# catch the results as a list of pandas dataframes\n",
    "res = Parallel(n_jobs=1)(delayed(return_pandas_df)() for i in range(3))\n",
    "# concatenate the list of data frames into a single one\n",
    "pd.concat(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
